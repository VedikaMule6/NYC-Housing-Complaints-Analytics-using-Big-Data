import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.sql.functions import col, when, lit, lpad

# Get job arguments
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

# Initialize Glue and Spark contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)


# Load 311 complaints data
df_311 = spark.read.option("header", "true").parquet(
    "s3://nyc-housing-complaints-g4/standardised_data/part-00000-b7170cff-e628-449c-b7e0-a66aa313906e-c000.snappy.parquet"
)

# Load HPD data
df_hpd = spark.read.option("header", "true").parquet(
    "s3://nyc-housing-complaints-g4/standardised_HPD_data/part-00000-9fb0f6ab-baa8-42b5-a22a-c361b49129d8-c000.snappy.parquet"
)

# Rename selected HPD columns
df_hpd = df_hpd.select([
    col(c).alias(f"{c}_hpd") if c in ['borough', 'street_name', 'full_address', 'latitude', 'longitude'] else col(c)
    for c in df_hpd.columns
])

# Extract relevant HPD columns
df_hpd_trimmed = df_hpd.select(
    col("unique_key").alias("hpd_unique_key"),
    col("bbl")
)

# Join 311 with HPD data on Unique_Key
df_joined = df_311.join(
    df_hpd_trimmed,
    df_311["Unique_Key"] == df_hpd_trimmed["hpd_unique_key"],
    how="left"
)

# Add validation flag
df_validated = df_joined.withColumn(
    "validation",
    when(col("hpd_unique_key").isNotNull(), lit(1)).otherwise(lit(0))
)

# Drop intermediate HPD join key
df_result = df_validated.drop("hpd_unique_key")

# Load PLUTO data
df_pluto = spark.read.option("header", "true").parquet(
    "s3://pluto311stnd/processed/part-00000-6c9bebab-487c-4ab9-b263-61b569bf118e-c000.snappy.parquet"
)

# Rename PLUTO columns
df_pluto = df_pluto \
    .withColumnRenamed("bbl", "bbl_pluto") \
    .withColumnRenamed("latitude", "latitude_pluto") \
    .withColumnRenamed("longitude", "longitude_pluto") \
    .withColumnRenamed("community_board", "community_board_pluto") \
    .withColumnRenamed("borough", "borough_pluto") \
    .withColumnRenamed("landmark", "landmark_pluto")

# Standardize BBL to 10-digit format for join
df_result = df_result.withColumn("bbl", lpad(col("bbl").cast("string"), 10, "0"))
df_pluto = df_pluto.withColumn("bbl_pluto", lpad(col("bbl_pluto").cast("string"), 10, "0"))

# Join with PLUTO data
df_final = df_result.join(
    df_pluto,
    df_result["bbl"] == df_pluto["bbl_pluto"],
    how="left"
).drop("bbl_pluto")

# Select final KPI columns
columns_to_keep = [
    # 311 complaint info
    'Unique_Key',
    'Created_Date', 'Closed_Date', 'created_date_stand', 'closed_date_stand',
    'complaint_type', 'Descriptor', 'complaint_category',
    'Status', 'validation',
    'borough', 'Incident_Zip', 'City', 'full_address',
    'Latitude', 'Longitude',

    # HPD BBL
    'bbl',

    # PLUTO property info
    'tax_block', 'tax_lot',
    'landuse', 'landuse_category',
    'bldgclass', 'ownertype', 'ownername',
    'lotarea', 'bldgarea', 'resarea', 'comarea',
    'unitsres', 'unitstotal',
    'numfloors', 'yearbuilt', 'yearalter1', 'yearalter2',
    'zonedist1', 'overlay1',
    'latitude_pluto', 'longitude_pluto',
    'bbl_standard'
]

df_kpi = df_final.select(columns_to_keep)

# Write final KPI DataFrame to S3
df_kpi.coalesce(1).write \
    .mode("overwrite") \
    .option("header", True) \
    .parquet("s3://glue311/abcd/")

# Commit Glue job
job.commit()
