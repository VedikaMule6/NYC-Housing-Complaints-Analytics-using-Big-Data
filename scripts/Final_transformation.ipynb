{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ2WMKhR5uWd"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import pyspark.sql\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.option(\"header\",\"true\").csv(\"s3://nyc-housing-complaints-g4/complaints-data/Housing_Complaints.csv\")"
      ],
      "metadata": {
        "id": "EHRxrxjA56jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"complaint_category\",\n",
        "    when(col(\"complaint type\").isin(\"heat/hot water\", \"heating\", \"heat/hot water\"), \"HEAT_ISSUE\")\n",
        "    .when(col(\"complaint type\").isin(\"water leak\", \"water drainage\"), \"WATER_LEAK\")\n",
        "    .when(col(\"complaint type\").isin(\"plumbing\", \"general construction/plumbing\", \"boilers\", \"boiler\"), \"PLUMBING_ISSUE\")\n",
        "    .when(col(\"complaint type\").isin(\"building condition\", \"structural\", \"unstable building\", \"building/use\"), \"BUILDING_CONDITION\")\n",
        "    .when(col(\"complaint type\").isin(\"electric\", \"electrical\", \"no power\"), \"ELECTRICAL_ISSUE\")\n",
        "    .when(col(\"complaint type\") == \"mold\", \"MOLD\")\n",
        "    .when(col(\"complaint type\").isin(\"elevator\"), \"ELEVATOR\")\n",
        "    .when(col(\"complaint type\").isin(\"paint - plaster\", \"paint/plaster\"), \"PAINT_PLASTER\")\n",
        "    .when(col(\"complaint type\").isin(\"door/window\"), \"DOOR_WINDOW\")\n",
        "    .when(col(\"complaint type\").isin(\"safety\", \"construction safety enforcement\", \"facade insp safety pgm\", \"best/site safety\", \"scaffold safety\"), \"SAFETY\")\n",
        "    .when(col(\"complaint type\").isin(\"construction\", \"general construction\", \"interior demo\"), \"CONSTRUCTION\")\n",
        "    .when(col(\"complaint type\").isin(\"miscellaneous categories\", \"quality of life\", \"lost property\", \"maintenance or facility\", \"forms\", \"outside building\", \"dob posted notice or order\", \"traffic signal condition\", \"borough office\", \"building marshal's office\", \"building marshals office\"), \"MISC\")\n",
        "    .when(col(\"complaint type\").isin(\"dept of investigations\", \"investigations and discipline (iad)\", \"forensic engineering\", \"executive inspections\", \"special projects inspection team (spit)\", \"special enforcement\", \"special operations\", \"ahv inspection unit\", \"special natural area district (snad)\", \"sustainability enforcement\"), \"INVESTIGATION\")\n",
        "    .when(col(\"complaint type\").isin(\"appliance\"), \"APPLIANCE\")\n",
        "    .when(col(\"complaint type\").isin(\"flooring/stairs\"), \"FLOORING_STAIRS\")\n",
        "    .when(col(\"complaint type\").isin(\"unsanitary condition\"), \"UNSANITARY_CONDITION\")\n",
        "    .when(col(\"complaint type\").isin(\"hpd literature request\"), \"EVICTION_LITERATURE\")\n",
        "    .otherwise(\"OTHER\")\n",
        ")"
      ],
      "metadata": {
        "id": "dERa1-BH6TLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hpd=spark.read.option(\"header\",\"true\").parquet(\"s3://nyc-housing-complaints-g4/standardised_HPD_data/part-00000-9fb0f6ab-baa8-42b5-a22a-c361b49129d8-c000.snappy.parquet\")"
      ],
      "metadata": {
        "id": "UhvfhDuOE1EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, lit\n",
        "\n",
        "# Step 1: Select only necessary columns from HPD\n",
        "df_hpd_trimmed = df_hpd.select(\n",
        "    col(\"unique_key\").alias(\"hpd_unique_key\"),\n",
        "    col(\"bbl\")\n",
        ")\n",
        "\n",
        "# Step 2: Left join on Unique_Key\n",
        "df_joined = df_311.join(\n",
        "    df_hpd_trimmed,\n",
        "    df_311[\"Unique_Key\"] == df_hpd_trimmed[\"hpd_unique_key\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Step 3: Add 'validation' column\n",
        "df_validated = df_joined.withColumn(\n",
        "    \"validation\",\n",
        "    when(col(\"hpd_unique_key\").isNotNull(), lit(1)).otherwise(lit(0))\n",
        ")\n",
        "\n",
        "# Step 4: Drop all HPD columns except 'bbl'\n",
        "df_result = df_validated.drop(\"hpd_unique_key\")\n",
        "\n",
        "# Final DataFrame has:\n",
        "# - All columns from df_311\n",
        "# - 'validation' column\n",
        "# - 'bbl' column from HPD\n",
        "\n"
      ],
      "metadata": {
        "id": "f73IZOsh8-qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pluto=spark.read.option(\"header\",\"true\").parquet(\"s3://pluto311stnd/processed/part-00000-6c9bebab-487c-4ab9-b263-61b569bf118e-c000.snappy.parquet\")"
      ],
      "metadata": {
        "id": "bLiERTvI_UF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pluto = df_pluto.withColumnRenamed(\"bbl\", \"bbl_pluto\")\n",
        "df_pluto = df_pluto.withColumnRenamed(\"latitude\", \"latitude_pluto\")\n",
        "df_pluto = df_pluto.withColumnRenamed(\"longitude\", \"longitude_pluto\")\n",
        "df_pluto = df_pluto.withColumnRenamed(\"community_board\", \"community_board_pluto\")\n",
        "df_pluto = df_pluto.withColumnRenamed(\"borough\", \"borough_pluto\")\n",
        "df_pluto = df_pluto.withColumnRenamed(\"landmark\", \"landmark_pluto\")"
      ],
      "metadata": {
        "id": "7usc9i-NDGRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lpad\n",
        "# Step 1: Standardize BBL data type (10-digit string)\n",
        "df_result = df_result.withColumn(\"bbl\", lpad(col(\"bbl\").cast(\"string\"), 10, \"0\"))\n",
        "df_pluto = df_pluto.withColumn(\"bbl_pluto\", lpad(col(\"bbl_pluto\").cast(\"string\"), 10, \"0\"))"
      ],
      "metadata": {
        "id": "KYzwPo22DQEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_result.join(\n",
        "    df_pluto,\n",
        "    df_result[\"bbl\"] == df_pluto[\"bbl_pluto\"],\n",
        "    how=\"left\"\n",
        ")"
      ],
      "metadata": {
        "id": "0L-2gMPuDub1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_final.drop(\"bbl_pluto\")"
      ],
      "metadata": {
        "id": "iXntEgfQDyVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the useful KPI columns to retain\n",
        "columns_to_keep = [\n",
        "    # 311 complaint info\n",
        "    'Unique_Key',\n",
        "    'Created_Date', 'Closed_Date', 'created_date_stand', 'closed_date_stand',\n",
        "    'complaint_type', 'Descriptor', 'complaint_category',\n",
        "    'Status', 'validation',\n",
        "    'borough', 'Incident_Zip', 'City', 'full_address',\n",
        "    'Latitude', 'Longitude',\n",
        "\n",
        "    # HPD BBL\n",
        "    'bbl',\n",
        "\n",
        "    # PLUTO property info\n",
        "    'tax_block', 'tax_lot',\n",
        "    'landuse', 'landuse_category',\n",
        "    'bldgclass', 'ownertype', 'ownername',\n",
        "    'lotarea', 'bldgarea', 'resarea', 'comarea',\n",
        "    'unitsres', 'unitstotal',\n",
        "    'numfloors', 'yearbuilt', 'yearalter1', 'yearalter2',\n",
        "    'zonedist1', 'overlay1',\n",
        "    'latitude_pluto', 'longitude_pluto',\n",
        "    'bbl_standard'\n",
        "]\n",
        "\n",
        "# Create the trimmed DataFrame\n",
        "df_kpi = df_final.select(columns_to_keep)\n"
      ],
      "metadata": {
        "id": "I3bPCGegD-De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_master=df_kpi"
      ],
      "metadata": {
        "id": "wOoJasrXIg1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_aff=spark.read.option(\"header\",\"true\").csv(\"s3://affordable311/Affordable_Housing_Production_by_Building_20250803.csv\")"
      ],
      "metadata": {
        "id": "IMfqXMrLIj_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, sum as _sum, avg\n",
        "\n",
        "df_complaints_by_borough=df_master.groupBy(\"borough\").agg(\n",
        "    count(\"*\").alias(\"total_complaints\"),\n",
        "    _sum(col(\"validation\").cast(\"int\")).alias(\"validated_complaints\"),\n",
        "    avg(col(\"validation\").cast(\"int\")).alias(\"validation_rate\")\n",
        ")"
      ],
      "metadata": {
        "id": "YegZIXyQJHl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_master_enriched = df_master.join(\n",
        "    df_complaints_by_borough,\n",
        "    on=\"borough\",\n",
        "    how=\"left\"\n",
        ")"
      ],
      "metadata": {
        "id": "l_D7ALxhJdE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_aff_summary = df_aff.groupBy(\"Borough\").agg(\n",
        "    count(\"*\").alias(\"total_projects\"),\n",
        "    _sum(col(\"`Total Units`\").cast(\"int\")).alias(\"total_affordable_units\"),\n",
        "    avg(col(\"`Total Units`\").cast(\"int\")).alias(\"avg_units_per_project\")\n",
        ").orderBy(\"Borough\")\n"
      ],
      "metadata": {
        "id": "IAnVzG3jJ73S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper\n",
        "\n",
        "df_aff_summary = df_aff_summary.withColumn(\"borough\", upper(col(\"borough\")))\n",
        "df_master_enriched = df_master_enriched.withColumn(\"borough\", upper(col(\"borough\")))\n"
      ],
      "metadata": {
        "id": "elhNQ6NJKL6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fully_enriched = df_master_enriched.join(\n",
        "    df_aff_summary,\n",
        "    on=\"borough\",\n",
        "    how=\"left\"\n",
        ")"
      ],
      "metadata": {
        "id": "fYrjl19YKMxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_fully_enriched"
      ],
      "metadata": {
        "id": "2bYOvYx3KrNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp, datediff, when, col\n",
        "\n",
        "# Step 1: Convert string to timestamp\n",
        "df = df.withColumn(\n",
        "    \"created_date_stand\", to_timestamp(\"created_date_stand\", \"yyyy-MM-dd HH:mm:ss\")\n",
        ").withColumn(\n",
        "    \"closed_date_stand\", to_timestamp(\"closed_date_stand\", \"yyyy-MM-dd HH:mm:ss\")\n",
        ")\n",
        "\n",
        "# Step 2: Calculate resolution_time in days\n",
        "df = df.withColumn(\n",
        "    \"resolution_time\",\n",
        "    when(\n",
        "        col(\"created_date_stand\").isNotNull() &\n",
        "        col(\"closed_date_stand\").isNotNull() &\n",
        "        (col(\"closed_date_stand\") >= col(\"created_date_stand\")),\n",
        "        datediff(\"closed_date_stand\", \"created_date_stand\")\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "6RzIzyfyK8Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, current_date\n",
        "\n",
        "# Step 1: Convert yearbuilt to integer safely\n",
        "df = df.withColumn(\n",
        "    \"yearbuilt\", col(\"yearbuilt\").cast(\"int\")\n",
        ")\n",
        "\n",
        "# Step 2: Calculate building age only if yearbuilt is not null\n",
        "df = df.withColumn(\n",
        "    \"building_age\",\n",
        "    when(\n",
        "        col(\"yearbuilt\").isNotNull(),\n",
        "        year(current_date()) - col(\"yearbuilt\")\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "kRI-KLw5LFXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Filter out unresolved complaints (i.e., null resolution times)\n",
        "df_resolved = df.filter(col(\"resolution_time\").isNotNull())\n",
        "\n",
        "# Group by borough and calculate average resolution time\n",
        "df_avg_resolution_by_borough = df_resolved.groupBy(\"borough\").agg(\n",
        "    avg(\"resolution_time\").alias(\"avg_resolution_time\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "cuXbx9O5LdTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.join(df_avg_resolution_by_borough,on=\"borough\",how=\"left\")"
      ],
      "metadata": {
        "id": "5jjySQd5MEHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Count complaints per borough and category\n",
        "complaint_ranked = df.groupBy(\"borough\", \"complaint_category\") \\\n",
        "    .count() \\\n",
        "    .withColumn(\"rank\", F.row_number().over(Window.partitionBy(\"borough\").orderBy(F.desc(\"count\"))))\n",
        "\n",
        "# Get top 5 complaint types\n",
        "top5_complaints = complaint_ranked.filter(\"rank <= 5\") \\\n",
        "    .select(\"borough\", \"complaint_category\") \\\n",
        "    .withColumn(\"top5_complaint_in_borough\", F.lit(\"Yes\"))\n",
        "\n",
        "# Join back to main DF\n",
        "df = df.join(top5_complaints, on=[\"borough\", \"complaint_category\"], how=\"left\") \\\n",
        "    .withColumn(\"top5_complaint_in_borough\", F.when(F.col(\"top5_complaint_in_borough\").isNull(), \"No\").otherwise(\"Yes\"))\n"
      ],
      "metadata": {
        "id": "6KLbdxziMagT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "building_age_avg_df = df.groupBy(\"borough\") \\\n",
        "    .agg(F.avg(\"building_age\").alias(\"avg_building_age\"))\n",
        "\n",
        "df = df.join(building_age_avg_df, on=\"borough\", how=\"left\")"
      ],
      "metadata": {
        "id": "8zOvjSWDMgde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"created_date_stand\", \"closed_date_stand\", \"ownertype\", \"ownername\",\n",
        "             \"latitude_pluto\", \"longitude_pluto\", \"zonedist1\", \"overlay1\", \"bbl_standard\")\n"
      ],
      "metadata": {
        "id": "Jb5k-ok5MqoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(\"descriptor\")"
      ],
      "metadata": {
        "id": "Rk6Z5Fo9Mrr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").parquet(\"s3://311-nyc-dataset/tranformations/T2(1)/\")"
      ],
      "metadata": {
        "id": "9Mu0CQIrMu3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}